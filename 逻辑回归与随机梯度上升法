import numpy as np
import matplotlib.pyplot as plt

def loadDataSet():
    dataMat = []; labelMat = []
    fr = open(r'D:\资源\python入门\计划单\机器学习\machinelearninginaction\Ch05\testSet.txt')
    for line in fr.readlines():
        lineArr = line.strip().split()
        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])
        labelMat.append(int(lineArr[2]))
    return dataMat,labelMat

def sigmoid(inX):
    if inX>=0:
        return 1/(1+np.exp(-inX))
    else:
        return np.exp(inX)/(1+np.exp(inX))

def gradAscent(dataMatIn,classLabels):                   #全批量梯度上升法
    dataMatrix = np.mat(dataMatIn)  # convert to NumPy matrix
    labelMat = np.mat(classLabels).T  # convert to NumPy matrix
    m,n=np.shape(dataMatrix)
    alpha=0.001
    maxCycles=500
    weight=np.ones((n,1))
    for k in range(maxCycles):
        h=sigmoid(dataMatrix*weight)
        error=labelMat-h
        weight=weight+alpha*dataMatrix.T*error
    return weight

dataMat,labelMat=loadDataSet()
weight=gradAscent(dataMat,labelMat)

def plot(weights):
    weights=weights.getA()
    dataMat, labelMat = loadDataSet()
    dataMatrix = np.mat(dataMat)  # convert to NumPy matrix
    labelMat = np.mat(labelMat).T  # convert to NumPy matrix
    data=dataMatrix[:,1:3]
    x = np.arange(-3.0, 3.0, 0.1)
    y = (-weights[0] - weights[1] * x) / weights[2]
    plt.scatter(data[:,0].getA(),data[:,1].getA(),30*(labelMat+1).getA(),30*(labelMat+1).getA())
    plt.plot(x,y)

def stocGradAscent0(dataMatrix, classLabels):
    m,n = np.shape(dataMatrix)
    alpha = 0.01
    weights = np.ones(n)   #initialize to all ones
    for i in range(m):
        h = sigmoid(sum(dataMatrix[i]*weights))
        error = classLabels[i] - h
        weights = weights + alpha * error * dataMatrix[i]
    return weights

def stocGradAscent1(dataMatrix, classLabels, numIter=150):            #随机梯度上升法                  #150次其实是实验试出来的
    m,n = np.shape(dataMatrix)
    weights = np.ones(n)
    for j in range(numIter):
        dataIndex = list(range(m))
        for i in range(m):
            alpha = 4/(1.0+j+i)+0.0001    #调整alpha大小，因为越接近最优，步长越小越好，否则容易超，造成震荡   numIter可以任意设置
            randIndex = int(np.random.uniform(0,len(dataIndex)))#随机选取样本
            h = sigmoid(sum(dataMatrix[randIndex]*weights))
            error = classLabels[randIndex] - h
            weights = weights + alpha * error * dataMatrix[randIndex]
            del(dataIndex[randIndex])               #把选出来的删了
    return weights

#案例
def classifyVector(inX, weights):
    prob = sigmoid(sum(inX*weights))
    if prob > 0.5: return 1.0
    else: return 0.0

def colicTest():
    frTrain = open(r'D:\资源\python入门\计划单\机器学习\machinelearninginaction\Ch05\horseColicTraining.txt')
    frTest = open(r'D:\资源\python入门\计划单\机器学习\machinelearninginaction\Ch05\horseColicTest.txt')
    frTrain=frTrain.readlines()
    frTest=frTest.readlines()
    trainingSet = []
    trainingLabels = []
    for line in frTrain:
        currLine = line.strip().split('\t')
        lineArr =[]
        for i in range(21):
            lineArr.append(float(currLine[i]))
        trainingSet.append(lineArr)
        trainingLabels.append(float(currLine[21]))
    trainWeights = stocGradAscent1(np.array(trainingSet), trainingLabels, 1000)
    errorCount = 0
    numTestVec = 0.0
    for line in frTest:
        numTestVec += 1.0
        currLine = line.strip().split('\t')
        lineArr =[]
        for i in range(21):
            lineArr.append(float(currLine[i]))
        if int(classifyVector(np.array(lineArr), trainWeights))!= int(currLine[21]):
            errorCount += 1
    errorRate = (float(errorCount)/numTestVec)
    print ("the error rate of this test is: %f" % errorRate)
    return errorRate
