import numpy as np
import matplotlib.pyplot as plt

def loadSimpData():
    datMat = np.matrix([[ 1. ,  2.1],
                        [ 2 ,  1.1],
                        [ 1.3,  1. ],
                        [ 1. ,  1. ],
                        [ 2. ,  1. ]])
    classLabels = np.matrix([[1.0],
                            [1.0],
                            [-1.0],
                            [-1.0],
                            [1.0]])
    return datMat,classLabels

xMat,yMat=loadSimpData()

def showPlot(xMat,yMat):
    x=np.array(xMat[:,0])
    y=np.array(xMat[:,1])
    label=np.array(yMat)
    plt.scatter(x,y,c=label)
    plt.title('adoaboost')
    plt.show()

showPlot(xMat,yMat)

def Classify0(xMat,i,Q,S):
    re=np.ones((xMat.shape[0],1))
    if S=='lt':
        re[xMat[:,i] <=Q]=-1                     # Q是阈值，小于阈值则赋值为-1
    else:
        re[xMat[:,i] >Q]=-1
    return re



# 单层决策树是一种简单的决策树，也称为决策树桩。这里的需要强调的是，和前面学的决策树有点不同，最大的不同之处在于他的决策依据不同了，我们前面学的是通过熵进行决策，而这里的决策只是通过一个阈值进行决策，即数据如果大于这个值或者小于这个值就会分类为1或者-1，就这么简单，但是这个阈值怎么找才能错误率最低呢？这里使用遍历的方法，怎么遍历呢？很简单，首先给定的数据中肯定有一个最大值和最小值，我们先设定待选的阈值有几个，假定有10个，那么我就通过最大值和最小值的差除以10的结果就是步长了，第一个阈值从最小值开始即把阈值设置为最小值，然后进行分类并计算分类的错误的个数，然后阈值向前步进一个步长然后继续计算分类错误率，计算完所有的阈值以后，然后选择一个最小的分类错误率的阈值为这个决策树的阈值。
def get_Stump(xMat,yMat,D):                       #单层决策树
    m,n=xMat.shape
    Steps=10                                      # 设置步数，目的是在步数以内找到最优的决策树
    bestStump={}                                  # 用字典形式储存树桩信息
    bestClas=np.mat(np.zeros((m,1)))              # 初始化分类结果为0
    minE=np.inf                                   # 错误率先设置为最大
    for i in range(n):                            # 先遍历数据的所有特征，上面说计算是在Steps以内找到最优的，因此这个循环是步数
        Min=xMat[:,i].min()
        Max=xMat[:,i].max()
        stepSize=(Max-Min)/Steps
        for j in range(-1,int(Steps)+1):          # 遍历大于或者小于两种情况,保证误差率小于0.5
            for S in ['lt','gt']:                 # lt是less than,gt是greater than
                Q=(Min+j*stepSize)
                re=Classify0(xMat,i,Q,S)          # 计算分类结果
                err=np.mat(np.ones((m,1)))        # 准备计算错误率率，初始化误差
                err[re==yMat]=0                   # 分类正确赋值为0
                eca=D.T*err                       # 计算误差
                if eca<minE:                      # 如果误差比之前的还小则更新新返回值，反之继续循环直达循环结束，返回
                    minE=eca
                    bestClas=re.copy()
                    bestStump['特征值']=i
                    bestStump['阈值']=Q
                    bestStump['标致']=S
    return bestStump,minE,bestClas

D=np.mat(np.ones((5,1))/5)                        # 初始化样本权重
bestStump,minE,bestClas=get_Stump(xMat,yMat,D)

def Ada_train(xMat,yMat,maxC):
    weakClass=[]
    m=xMat.shape[0]
    D=np.mat(np.ones((m,1))/m)                                                #初始化权重
    aggClass=np.mat(np.zeros((m,1)))
    for i in range(maxC):
        Stump,error,bestClas=get_Stump(xMat,yMat,D)
        alpha=float(0.5*np.log((1-error)/max(error,1e-16)))                  #计算决策树权值
        Stump['alpha']=np.round(alpha,2)
        weakClass.append(Stump)                                                 #储存单层决策树
        expon=np.multiply(-1*alpha*yMat,bestClas)
        D=np.multiply(D,np.exp(expon))
        D=D/D.sum()
        aggClass += alpha*bestClas
        aggErr=np.multiply(np.sign(aggClass) != yMat, np.ones((m,1)))
        errRate=aggErr.sum()/m
        if errRate==0.0:
            break
    return weakClass,aggClass

weakClass,aggClass=Ada_train(xMat,yMat,40)

def AdaClassify(data,weakClass):                     #测试
    dataMat=np.mat(data)
    m=dataMat.shape[0]
    aggClass=np.mat(np.zeros((m,1)))
    for i in range(len(weakClass)):
        classEst=Classify0(dataMat,weakClass[i]['特征值'],weakClass[i]['阈值'],weakClass[i]['标致'])
        aggClass +=weakClass[i]['alpha']*classEst
    return np.sign(aggClass)
