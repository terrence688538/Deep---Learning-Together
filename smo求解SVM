import numpy as np
import matplotlib.pyplot as plt

def loadDataSet(fileName):
    dataMat = []
    labelMat = []
    fr = open(fileName)
    fr=fr.readlines()
    for line in fr:
        lineArr = line.strip().split('\t')
        dataMat.append([float(lineArr[0]), float(lineArr[1])])
        labelMat.append(float(lineArr[2]))
    return dataMat,labelMat

def showDataSet(dataArr,labelArr):
    datamat=np.array(dataArr)
    labelArr=np.array(labelArr)
    plt.scatter(datamat[:,0],datamat[:,1],15*(labelArr+2),15*(labelArr+2))

def selectJrand(i,m):               #选择aerfa对
    j=i #we want to select any J not equal to i
    while (j==i):                               #只要相同就一直循环
        j = int(np.random.uniform(0,m))         #uniform是可以小数的，我们需要整数
    return j

def clipAlpha(aj,H,L):
    if aj > H:
        aj = H
    if L > aj:
        aj = L
    return aj

def smoSimple(dataMatIn, classLabels, C, toler, maxIter):
    dataMatrix=np.mat(dataMatIn)
    labelMat=np.mat(classLabels).T
    b=0
    m,n=np.shape(dataMatrix)
    iter=0
    alphas=np.mat(np.zeros((m,1)))                 #初始化
    while (iter<maxIter):
        alphaPairsChanged = 0
        for i in range(m):
            fXi=float(np.multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T))+b
            Ei=fXi-float(labelMat[i])                               #计算误差
            if ((labelMat[i] * Ei < -toler) and (alphas[i] < C)) or ((labelMat[i] * Ei > toler) and (alphas[i] > 0)):
                j=selectJrand(i,m)
                fXj = float(np.multiply(alphas, labelMat).T * (dataMatrix * dataMatrix[j, :].T)) + b
                Ej=fXj-float(labelMat[j])
                alphaIold=alphas[i].copy()         #alphaI_old
                alphaJold=alphas[j].copy()         #alphaJ_old
                if (labelMat[i] !=labelMat[j]):                 #看alpha是否在上下界内
                    L=max(0,alphas[j]-alphas[i])
                    H=min(C,C+alphas[j]-alphas[i])
                else:
                    L=max(0,alphas[j]+alphas[i]-C)
                    H=min(C,alphas[j]-alphas[i])
                if L==H:
                    continue
                eta=2*dataMatrix[i,:]*dataMatrix[j,:].T-dataMatrix[i,:]*dataMatrix[i,:].T-dataMatrix[j,:]*dataMatrix[j,:].T    #计算学习率
                if eta>=0:          #看：这里是课本上学习率取负数，，，，书本上的eta必然大于0,由正定核函数性质得
                    continue
                alphas[j] -=labelMat[j]*(Ei-Ej)/eta            #eta是负的，所以这里是减号
                alphas[j]=clipAlpha(alphas[j],H,L)             #判断alphas的界限
                if abs(alphas[j]-alphaJold)<0.00001:
                    continue
                alphas[i] +=labelMat[i]*labelMat[j]*(alphaJold-alphas[j])
                b1=b-Ei-labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T-labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T
                b2=b-Ej-labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T-labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T
                if (0<alphas[i]) and (C>alphas[i]):
                    b=b1
                elif (0<alphas[j]) and (C>alphas[j]):
                    b=b2
                else:
                    b=(b1+b2)/2
                alphaPairsChanged += 1
        if alphaPairsChanged ==0:
            iter += 1
        else:
            iter=0
    return b,alphas

def get_sv(dataArr,labelArr,alpha):
    dataMatrix = np.array(dataArr)
    m=dataMatrix.shape[0]
    sv_x=[]
    sv_y=[]
    for i in range(m):
        if alpha[i]>0:
            sv_x.append(dataMatrix[i])
            sv_y.append(labelArr[i])
    sv_X=np.array(sv_x).T
    sv_Y=np.array(sv_y)
    return sv_X,sv_Y

def showDataSet_2(dataArr,labelArr,alpha):
    datamat=np.array(dataArr)
    labelArr=np.array(labelArr)
    plt.scatter(datamat[:,0],datamat[:,1],15*(labelArr+2),15*(labelArr+2))      #利用样本标签实现画图，，第一个15*(labelArr+2)是大小，后一个是颜色
    sv_X, sv_Y = get_sv(dataArr, labelArr, alpha)
    plt.scatter(sv_X[0],sv_X[1],s=150,c='none',alpha=0.7,linewidths=1.5,edgecolors='red')  #alpha=0.7透明度   linewidths=1.5宽度    edgecolors='red'边框的颜色


# kernelEval = kernelTrans(sVs,datMat[i,:],('rbf', k1))
# 从这一句我们看到传入的第一个参数X就是sVs，往上追溯可知这是参与內积的训练数据点即支持向量
# 第二个参数A，传入的是datMat[i:0]，传入的是一条数据，即待分类的一个数据，原因是每个待分类的数据都需要和支持向量相內积
# kTup[0]第一个元素是选择核函数类型的参数，第二个kTup[1]就更简单了，其实就是径向基的那个参数也就是计算公式中的方差，大家看这个就懂了
#   K = exp(K / (-1 * kTup[1] ** 2))
def kernelTrans(X,A,kTup):
    X=np.mat(X)
    m,n = np.shape(X)
    K = np.mat(np.zeros((m,1)))
    if kTup[0]=='lin':
        K = X * A.T
    elif kTup[0]=='rbf':
        for j in range(m):
            deltaRow = X[j,:] - A
            K[j] = deltaRow*deltaRow.T
        K = np.exp(K/(-1*kTup[1]**2))
    else:
        raise NameError('核函数无法识别')
    return K

#完整版不带核函数
class optStruct:
    def __init__(self,dataMatIn, classLabels, C, toler,kTup):  # Initialize the structure with the parameters
        self.X = dataMatIn
        self.Y = classLabels
        self.C = C
        self.tol = toler
        self.m = np.shape(dataMatIn)[0]
        self.alphas = np.mat(np.zeros((self.m,1)))
        self.b = 0
        self.eCache = np.mat(np.zeros((self.m,2))) #缓存误差            第二列是误差   第一列为是否有效的标志位
        self.K=np.mat(np.zeros((self.m,self.m)))
        for i in range(self.m):
            self.K[:,i]=kernelTrans(self.X, self.X[i,:], kTup)

def calcEk(oS,k):
    fXk = float(np.multiply(oS.alphas,oS.Y).T * oS.K[:,k]) + oS.b
    Ek = fXk - float(oS.Y[k])
    return Ek

#启发式
def selectJK(i, oS, Ei):
    maxK = -1                               # 定义使的计算E的差值最大
    maxDeltaE = 0                           # Ei-Ej  用于E值的最大变化量
    Ej = 0                                  # 初始化
    oS.eCache[i] = [1,Ei]                   # 根据Ei更新误差缓存
    eca = np.nonzero(oS.eCache[:,0].A)[0]   # 返回误差不为0的数据的索引
    # 其中.A的意思是把矩阵转换成数组，因为oS.eCache[:, 0].A，取的是一列的数据E值有效位，转换后还是一列
    # 而nonzero(oS.eCache[:, 0].A)[0]返回的是非零的目录列表值并取出赋给eca，共下面处理
    if (len(eca)) > 1:
        for k in eca:                       # 索引eca的标号，求最大差值
            if k == i:                      # 如果相等，说明选取两个alpha一样，直接跳出本次循环，进行下一次循环
                continue
            Ek = calcEk(oS, k)              # 计算Ek
            deltaE = abs(Ei - Ek)           # 求两个E的差值，选择最大的步长
            if (deltaE > maxDeltaE):        # 比较，选择最大的差值
                maxK = k
                maxDeltaE = deltaE
                Ej = Ek
        return maxK, Ej
    # 这个是为第一次准备的，不满足上面的条件即(len(validEcacheList)) > 1，因为第一次的E都为0，
    # 所以长度都为0，不满足条件，来到这里，进行随机抽取alpha 的 ij
    else:
        j = selectJrand(i, oS.m)
        Ej = calcEk(oS, j)
    return j, Ej

def updateEk(oS, k):#after any alpha has changed update the new value in the cache
    Ek = calcEk(oS, k)
    oS.eCache[k] = [1,Ek]

def inner(i,oS):
    Ei=calcEk(oS,i)
    if ((oS.Y[i] * Ei < -oS.tol) and (oS.alphas[i] < oS.C)) or ((oS.Y[i] * Ei > oS.tol) and (oS.alphas[i] > 0)):
        j,Ej=selectJK(i,oS,Ei)                 #选择并计算Ej
        alphaIold = oS.alphas[i].copy()  # alphaI_old
        alphaJold = oS.alphas[j].copy()  # alphaJ_old
        if (oS.Y[i] != oS.Y[j]):  # 看alpha是否在上下界内
            L = max(0, oS.alphas[j] - oS.alphas[i])
            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])
        else:
            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)
            H = min(oS.C, oS.alphas[j] - oS.alphas[i])
        if L == H:
            return 0
        eta = 2 * oS.K[i,j] - oS.K[i,i] - oS.K[j,j]  # 计算学习率
        if eta >= 0:  # 看：这里是课本上学习率取负数，，，，书本上的eta必然大于0,由正定核函数性质得
            return 0
        oS.alphas[j] -= oS.Y[j] * (Ei - Ej) / eta  # eta是负的，所以这里是减号
        oS.alphas[j] = clipAlpha(oS.alphas[j], H, L)  # 判断alphas的界限
        updateEk(oS,j)
        if abs(oS.alphas[j] - alphaJold) < 0.00001:
            return 0
        oS.alphas[i] += oS.Y[i] * oS.Y[j] * (alphaJold - oS.alphas[j])
        updateEk(oS, i)
        b1 = oS.b - Ei - oS.Y[i] * (oS.alphas[i] - alphaIold) * oS.K[i,i] - oS.Y[j] * (oS.alphas[j] - alphaJold) * oS.K[i,j]
        b2 = oS.b - Ej - oS.Y[i] * (oS.alphas[i] - alphaIold) * oS.K[i,j] - oS.Y[j] * (oS.alphas[j] - alphaJold) * oS.K[j,j]
        if (0 < oS.alphas[i]) and (oS.C > oS.alphas[i]):
            oS.b = b1
        elif (0 < oS.alphas[j]) and (oS.C > oS.alphas[j]):
            oS.b = b2
        else:
            oS.b = (b1 + b2) / 2
        return 1
    else:
        return 0

def smoP(dataMatIn, classLabels, C, toler, maxIter,kTpu=('lin',0)):                   #外循环
    oS = optStruct(np.mat(dataMatIn),np.mat(classLabels).transpose(),C,toler,kTpu)    # 初始化数据结构
    iter = 0
    entireSet = True            #控制循环用的
    alphaPairsChanged = 0           #记录更新次数
    # 遍历整个数据集alpha都没有更新或者超过最大迭代次数，则退出循环
    while (iter < maxIter) and ((alphaPairsChanged > 0) or (entireSet)):
        alphaPairsChanged = 0
        if entireSet:                                         # 遍历整个数据集
            for i in range(oS.m):
                alphaPairsChanged += inner(i,oS)
            iter += 1
        else:                                                # 遍历不在边界0和C的alpha
            nonBoundIs = np.nonzero((oS.alphas.A > 0) * (oS.alphas.A < C))[0]
            for i in nonBoundIs:
                alphaPairsChanged += inner(i,oS)
            iter += 1
        if entireSet:                                                            # 遍历一次后改为非边界遍历
            entireSet = False
        elif (alphaPairsChanged == 0):                                           # 如果alpha没有更新，计算全样本遍历
            entireSet = True
    return oS.b,oS.alphas

def calcWs(alpha,dataArr,labelArr):
    dataMatrix = np.mat(dataArr)
    labelMat = np.mat(labelArr).T
    m, n = np.shape(dataMatrix)
    w=np.zeros((n,1))
    for i in range(m):
        w += np.multiply(alpha[i]*labelMat[i],dataMatrix[i,:].T)
    return w

def showDataSet_3(dataArr,labelArr,alpha,w):
    datamat=np.array(dataArr)
    labelArr=np.array(labelArr)
    plt.scatter(datamat[:,0],datamat[:,1],15*(labelArr+2),15*(labelArr+2))      #利用样本标签实现画图，，第一个15*(labelArr+2)是大小，后一个是颜色
    sv_X, sv_Y = get_sv(dataArr, labelArr, alpha)
    plt.scatter(sv_X[0],sv_X[1],s=150,c='none',alpha=0.7,linewidths=1.5,edgecolors='red')  #alpha=0.7透明度   linewidths=1.5宽度    edgecolors='red'边框的颜色
    xData=(1,6)
    k = (-w[0] / w[1])
    d = (-b.A[0] / w[1])
    plt.plot(xData,k*xData+d ,'r')

def calcAcc(dataArr,labelArr,w,b):
    dataMatrix = np.mat(dataArr)
    labelMat = np.mat(labelArr).T
    m, n = np.shape(dataMatrix)
    yhat=[]
    re=0
    for i in range(m):
        result=dataMatrix[i]*np.mat(w)+b
        if result<0:
            yhat.append(-1)
        else:
            yhat.append(1)
        if yhat[i] == labelMat[i]:
            re +=1
    acc=re/m
    return acc

def testRbf(k1=1.3):
    dataArr,labelArr = loadDataSet(r'D:\资源\python入门\计划单\机器学习\machinelearninginaction\Ch06\testSetRBF.txt')
    b,alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, ('rbf', k1))
    datMat=np.mat(dataArr)
    labelMat = np.mat(labelArr).transpose()
    svInd=np.nonzero(alphas.A>0)[0]               #获取支持向量的索引
    sVs=datMat[svInd]                             #获得支持向量
    labelSV = labelMat[svInd]                     #获得支持向量的标签
    print ("there are %d Support Vectors" % np.shape(sVs)[0])
    m,n = np.shape(datMat)
    errorCount = 0
    for i in range(m):
        kernelEval = kernelTrans(sVs,datMat[i,:],('rbf', k1))
        predict=kernelEval.T * np.multiply(labelSV,alphas[svInd]) + b
        if np.sign(predict)!=np.sign(labelArr[i]): errorCount += 1
    print ("the training error rate is: %f" % (float(errorCount)/m))
    dataArr,labelArr = loadDataSet(r'D:\资源\python入门\计划单\机器学习\machinelearninginaction\Ch06\testSetRBF.txt')
    errorCount = 0
    datMat=np.mat(dataArr); labelMat = np.mat(labelArr).transpose()
    m,n = np.shape(datMat)
    for i in range(m):
        kernelEval = kernelTrans(sVs,datMat[i,:],('rbf', k1))          #此处的支持向量是根据训练集得出的
        predict=kernelEval.T * np.multiply(labelSV,alphas[svInd]) + b   #此处的支持向量的标签，alphas,b都是根据训练集计算出的
        if np.sign(predict)!=np.sign(labelArr[i]): errorCount += 1
    print ("the test error rate is: %f" % (float(errorCount)/m) )

dataArr,labelArr=loadDataSet(r'D:\资源\python入门\计划单\机器学习\machinelearninginaction\Ch06\testSetRBF2.txt')
b,alphas=smoSimple(dataArr,labelArr,0.6,0.001,5)
sv_X,sv_Y=get_sv(dataArr,labelArr,alphas)
b,alpha=smoP(dataArr,labelArr,0.6,0.001,500,kTpu=('lin',0))
w=calcWs(alpha,dataArr,labelArr)
acc=calcAcc(dataArr,labelArr,w,b)
showDataSet_3(dataArr,labelArr,alpha,w)

dataArr,labelArr=loadDataSet(r'D:\资源\python入门\计划单\机器学习\machinelearninginaction\Ch06\testSetRBF.txt')
showDataSet(dataArr,labelArr)
